# ðŸ§  AI Resume Analyzer â€” Node.js + Express REST API

A production-ready backend for analyzing PDF resumes using text extraction and AI scoring.

---

## âœ¨ Features

- **PDF Upload** â€” Multipart form-data upload with type and size validation
- **Text Extraction** â€” Parses PDF content using `pdf-parse`
- **MongoDB Persistence** â€” Stores resume metadata, extracted text, and analysis
- **AI Analysis** â€” Mock analysis engine (scoring, keywords, section detection) with clear swap-in points for OpenAI/Gemini
- **MVC Architecture** â€” Clean separation of concerns across Models, Views (responses), Controllers
- **Security** â€” Helmet, CORS, rate limiting, input sanitization
- **Error Handling** â€” Centralized global error handler with operational vs. programmer error classification
- **Logging** â€” Winston-powered structured logging (console + file)
- **Graceful Shutdown** â€” Handles `SIGTERM`/`SIGINT` cleanly
- **Tests** â€” Jest + Supertest integration test suite

---

## ðŸ“ Project Structure

```
ai-resume-analyzer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ database.js        # MongoDB connection with reconnect handling
â”‚   â”‚   â””â”€â”€ multer.js          # File upload configuration
â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â””â”€â”€ resumeController.js # MVC controllers for all resume operations
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ errorHandler.js    # Global error handler
â”‚   â”‚   â””â”€â”€ validateMongoId.js # ObjectId validation middleware
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ Resume.js          # Mongoose schema + model
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ healthRoutes.js    # Health check endpoint
â”‚   â”‚   â””â”€â”€ resumeRoutes.js    # Resume CRUD + analyze routes
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ aiService.js       # Mock AI analysis (swap-in ready for real AI)
â”‚   â”‚   â””â”€â”€ pdfService.js      # PDF text extraction logic
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ AppError.js        # Custom operational error class
â”‚   â”‚   â”œâ”€â”€ asyncHandler.js    # Async wrapper to eliminate try/catch
â”‚   â”‚   â””â”€â”€ logger.js          # Winston logger
â”‚   â”œâ”€â”€ app.js                 # Express app (middleware + routes)
â”‚   â””â”€â”€ server.js              # Entry point (DB connect + server start)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ resume.test.js         # Integration test suite
â”œâ”€â”€ uploads/                   # Uploaded PDFs (gitignored)
â”œâ”€â”€ logs/                      # Log files (auto-created)
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ jest.config.json
â””â”€â”€ package.json
```

---

## ðŸš€ Getting Started

### Prerequisites

- Node.js >= 18
- MongoDB (local or Atlas)

### 1. Clone & Install

```bash
git clone <repo-url>
cd ai-resume-analyzer
npm install
```

### 2. Configure Environment

```bash
cp .env.example .env
```

Edit `.env`:

```env
PORT=5000
NODE_ENV=development
MONGODB_URI=mongodb://localhost:27017/resume_analyzer
MAX_FILE_SIZE_MB=5
```

### 3. Run

```bash
# Development (auto-restart on file changes)
npm run dev

# Production
npm start
```

Server starts at: `http://localhost:5000`

### 4. Run Tests

```bash
npm test
```

---

## ðŸ“¡ API Reference

### Health Check

```
GET /api/health
```

**Response:**
```json
{
  "status": "success",
  "environment": "development",
  "uptime": "42s",
  "database": "connected"
}
```

---

### Upload Resume

```
POST /api/resumes/upload
Content-Type: multipart/form-data
Field: resume (PDF file, max 5MB)
```

```bash
curl -X POST http://localhost:5000/api/resumes/upload \
  -F "resume=@/path/to/your/resume.pdf"
```

**Response (201):**
```json
{
  "status": "success",
  "message": "Resume uploaded and text extracted successfully.",
  "data": {
    "resume": {
      "_id": "66a1234abc...",
      "originalName": "john-doe-resume.pdf",
      "status": "processed",
      "wordCount": 487,
      "pageCount": 1
    }
  }
}
```

---

### Analyze Resume

```
POST /api/resumes/:id/analyze
Content-Type: application/json
Body (optional): { "jobDescription": "Looking for a senior Node.js developer..." }
```

```bash
curl -X POST http://localhost:5000/api/resumes/66a1234abc/analyze \
  -H "Content-Type: application/json" \
  -d '{"jobDescription": "React and Node.js engineer"}'
```

**Response (200):**
```json
{
  "status": "success",
  "data": {
    "analysis": {
      "overallScore": 82,
      "experienceLevel": "senior",
      "strengths": ["Contact information present", "Strong keyword density"],
      "weaknesses": [],
      "suggestions": ["Add a professional summary"],
      "keywords": ["javascript", "react", "node", "docker", "aws"],
      "sections": {
        "hasContact": true,
        "hasSummary": false,
        "hasExperience": true,
        "hasEducation": true,
        "hasSkills": true
      }
    }
  }
}
```

---

### List All Resumes

```
GET /api/resumes?page=1&limit=10
```

---

### Get Single Resume

```
GET /api/resumes/:id
```

---

### Delete Resume

```
DELETE /api/resumes/:id
```

---

## ðŸ¤– Integrating Real AI

The `aiService.js` file is designed with a clear swap-in point. To connect OpenAI:

```bash
npm install openai
```

Replace the body of `analyzeResume()` in `src/services/aiService.js`:

```js
const OpenAI = require('openai');
const client = new OpenAI({ apiKey: process.env.AI_API_KEY });

const analyzeResume = async (resumeText, options = {}) => {
  const completion = await client.chat.completions.create({
    model: process.env.AI_MODEL || 'gpt-4',
    messages: [
      {
        role: 'system',
        content: 'You are an expert resume reviewer. Return JSON only.',
      },
      {
        role: 'user',
        content: `Analyze this resume and return JSON with keys:
          overallScore (0-100), strengths (array), weaknesses (array),
          suggestions (array), keywords (array), experienceLevel (entry/mid/senior/executive).\n\n${resumeText}`,
      },
    ],
    response_format: { type: 'json_object' },
  });
  return JSON.parse(completion.choices[0].message.content);
};
```

---

## ðŸ”’ Security Considerations for Production

- Set `CORS_ORIGIN` to your frontend domain (not `*`)
- Use MongoDB Atlas with auth credentials in `MONGODB_URI`
- Store uploaded files in S3/GCS instead of local disk
- Add authentication middleware (JWT) before protected routes
- Enable HTTPS via a reverse proxy (nginx/Caddy)
